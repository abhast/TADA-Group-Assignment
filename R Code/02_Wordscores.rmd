---
title: "Wordscore"
authors: Abhas, Anna, Maria, Sofia
output: html_notebook
---

Following chunk 
- reads Wikipedia page content for all the legislators selected above
- parses the content into readable text, removing line breaks
- creats a text file for each legislator containing clean page content
#Custom list of legislators (reference text)
```{r}
legislator_name_list_ref <- c("Bernie_Sanders", "Elizabeth_Warren", "Maxine_Waters", "Tim_Scott", "Liz_Cheney", 
                              "Mitch_McConnell", "Michele_Bachmann", "Ted_Cruz", "Alexandria_Ocasio-Cortez", 
                              "John_Lewis_(civil_rights_leader)", "Tim_Johnson_(South_Dakota_politician)", 
                              "Steve_Scalise", "Ed_Royce", "Martha_McSally", "Mark_Walker_(North_Carolina_politician)", 
                              "David_Valadao", "Tom_McClintock", "Steven_Palazzo", "Mick_Mulvaney", "Martha_Roby", 
                              "Orrin_Hatch", "Elijah_Cummings", "Katherine_Clark", "RaÃºl_Grijalva", "Keith_Ellison", 
                              "Chris_Murphy", "Sheila_Jackson_Lee", "Mark_Pocan", "Carol_Shea-Porter", "Jerrold_Nadler",
                              "Ron_Paul", "Yvette_Clarke")


for (i in legislator_name_list_ref){
  #print(paste0("Loading Page for: ", i))
  web_address <- paste0("https://en.wikipedia.org/wiki/", i)
  page_html <- read_html(web_address)
  page_paragraphs <- html_nodes(page_html,"p")
  page_text <- paste(html_text(page_paragraphs), sep = '', collapse = '')
  
  text_data <- as.null()
  
  for (j in 1:length(page_text)) {
    if (is.null(text_data)) { text_data <- page_text }
    else { text_data <- paste(text_data, page_text, sep = '', collapse = '') }
  }
  
  # text pre-processing
  text_data <- gsub("\n", " ", text_data)       # remove the existing line breaks
  text_data <- gsub("\\[\\d*\\]","", text_data) # removes citations
  text_data <- gsub("U\\.S\\.","US", text_data) # changes all "U.S." occurences to "US"
 # text_data <- gsub("[[:upper:]]\\.", "[[:upper:]]", text_data)
  
  if(!dir.exists("/cloud/project/Wiki Reference Profiles WS/")){
    dir.create("Wiki Reference Profiles WS")
  }
  
  file_name <- gsub("\\_\\(.*","", i)
  
  if(str_count(file_name,"_")==1){
    file_name <- gsub("\\_","_NA_", file_name)
  }

  write(text_data, paste0("/cloud/project/Wiki Reference Profiles WS/", paste0(file_name, ".txt")))
  
  print(paste0("Created file: ", file_name))

}

```


Following chunk-
- reads all the text files created above
- creates a data corpus
#Creating corpus, DFM
```{r}
wiki_files <- readtext("Wiki Reference Profiles WS/*.txt", 
                       docvarsfrom = "filenames", 
                       dvsep = "_", 
                       docvarnames = c("FirstName", "MiddleName", "LastName"))
wiki_corpus <- corpus(wiki_files, text_field = "text")

wiki_token <- tokens(wiki_corpus, what="word")

wiki_dfm <- dfm(wiki_corpus, 
                stem = TRUE, 
                remove = stopwords("english"), 
                remove_punct = TRUE, 
                remove_numbers = TRUE)


```


#Corpus summary, subset, filtering based on Tokens and Types
```{r}
summary(wiki_corpus)
corpus_summary <- summary(wiki_corpus)
corpus_summary$Tokens
order(wiki_corpus)
summary(corpus_subset(wiki_corpus, FirstName=="Bernie"))

ntoken(wiki_corpus)
ntype(wiki_corpus)

summary(corpus_subset(wiki_corpus, ntoken(wiki_corpus)>6000))
summary(corpus_subset(wiki_corpus, ntype(wiki_corpus)>3000))

high_token_corpus <- corpus_subset(wiki_corpus, ntoken(wiki_corpus)>10000)

summary(high_token_corpus)
```


#Exploring corpus text
```{r}
kwic(wiki_corpus, pattern = "terror")
kwic(wiki_corpus, pattern = "terror", valuetype = "regex")
kwic(wiki_corpus, pattern = "abort*")

kwic(wiki_corpus, pattern = phrase("health"))

textplot_xray(
    kwic(wiki_corpus, pattern = "terror")) + 
    ggtitle("Lexical dispersion")
textplot_xray(
    kwic(wiki_corpus, pattern = "abort*")) + 
    ggtitle("Lexical dispersion")

wiki_token <- tokens(wiki_corpus, what="word")
col_toks <- wiki_token %>% 
       tokens_remove(stopwords("en")) %>% 
       tokens_select(pattern = "^[A-Z]", valuetype = "regex", 
                     case_insensitive = FALSE, padding = TRUE) %>% 
       textstat_collocations(min_count = 5, tolower = FALSE)
head(col_toks)
col_toks

```


This chunk explores the lexical diversity between Wikipedia page content for Bernie Sanders and Orrin Hatch.
#Exploring lexical diversity
```{r}
docvars(wiki_corpus)
Bernie <- corpus_subset(wiki_corpus, FirstName == "Bernie")
B_token <- tokens(Bernie, what="word")
B_toks <- B_token %>% 
       tokens_remove(stopwords("en")) %>% 
       tokens_select(pattern = "^[A-Z]", valuetype = "regex", 
                     case_insensitive = FALSE, padding = TRUE) %>% 
       textstat_collocations(min_count = 5, tolower = FALSE)
B_toks

Orrin <- corpus_subset(wiki_corpus, FirstName == "Orrin")
O_token <- tokens(Orrin, what="word")
O_toks <- O_token %>% 
       tokens_remove(stopwords("en")) %>% 
       tokens_select(pattern = "^[A-Z]", valuetype = "regex", 
                     case_insensitive = FALSE, padding = TRUE) %>% 
       textstat_collocations(min_count = 5, tolower = FALSE)
O_toks

lexdiv <- textstat_lexdiv(wiki_dfm)
lexdiv

read <- textstat_readability(wiki_corpus, measure = "Flesch")
read

textstat_dist(wiki_dfm, method="euclidean")
textstat_simil(wiki_dfm, method="cosine")
textstat_simil(wiki_dfm, method="hamman")

Bernie_dfm <- dfm(Bernie, remove = stopwords("en"), tolower=T, stem=T, remove_punct=T)
topfeatures(Bernie_dfm)
textplot_wordcloud(Bernie_dfm)

Orrin_dfm <- dfm(Orrin, remove = stopwords("en"), tolower=T, stem=T, remove_punct=T)
topfeatures(Orrin_dfm)
textplot_wordcloud(Orrin_dfm)

docnames(wiki_dfm)
head(textstat_keyness(wiki_dfm, target="Bernie_NA_Sanders.txt",
                      measure="chi2"), n=20)
textstat_keyness(wiki_dfm, target="Bernie_NA_Sanders.txt",
                      measure="chi2") %>% textplot_keyness()
head(textstat_keyness(wiki_dfm, target="Orrin_NA_Hatch.txt",
                      measure="chi2"), n=20)
textstat_keyness(wiki_dfm, target="Orrin_NA_Hatch.txt",
                      measure="chi2") %>% textplot_keyness()

topfeatures(wiki_dfm)
textplot_wordcloud(wiki_dfm, min_count=5, max_words=500)

```


# -------- Method 1: Applying Wordscores to the corpus

#Variation 1: Sanders-Bachmann
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of Bernie Sanders and Michele Bachman wiki content
dem_index <- which((wiki_data$FirstName == 'Bernie' & wiki_data$LastName == 'Sanders'))
rep_index <- which((wiki_data$FirstName == 'Michele' & wiki_data$LastName == 'Bachmann'))

# set the reference scrores for Bernie and Michele and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

refscores

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

We run multiple models, for better fits.
#Variation 2: Sanders-Hatch
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of Bernie Sanders and Orrin Hatch wiki content
dem_index <- which((wiki_data$FirstName == 'Bernie' & wiki_data$LastName == 'Sanders'))
rep_index <- which((wiki_data$FirstName == 'Orrin' & wiki_data$LastName == 'Hatch'))

# set the reference scrores for Bernie and Orrin and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

Variation #2 is a better fit than Sanders-Bachmann, which leads us to believe that profile length is really a factor when applying Wordscores to Wikipedia. Still, we'll run a couple more variations. 

#Variation 3: Sanders-Cruz
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of Bernie Sanders and Ted Cruz wiki content
dem_index <- which((wiki_data$FirstName == 'Bernie' & wiki_data$LastName == 'Sanders'))
rep_index <- which((wiki_data$FirstName == 'Ted' & wiki_data$LastName == 'Cruz'))

# set the reference scrores for Bernie and Ted and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

#Variation 4: AOC-Bachmann
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of AOC and Michele Bachmann wiki content
dem_index <- which((wiki_data$FirstName == 'Alexandria' & wiki_data$LastName == 'Ocasio-Cortez'))
rep_index <- which((wiki_data$FirstName == 'Michele' & wiki_data$LastName == 'Bachmann'))

# set the reference scrores for AOC and Bachmann and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

#Variation 5: AOC-Hatch
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of AOC and Orrin Hatch wiki content
dem_index <- which((wiki_data$FirstName == 'Alexandria' & wiki_data$LastName == 'Ocasio-Cortez'))
rep_index <- which((wiki_data$FirstName == 'Orrin' & wiki_data$LastName == 'Hatch'))

# set the reference scrores for AOC and Hatch and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

#Variation 6: AOC-Cruz
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of AOC and Ted Cruz wiki content
dem_index <- which((wiki_data$FirstName == 'Alexandria' & wiki_data$LastName == 'Ocasio-Cortez'))
rep_index <- which((wiki_data$FirstName == 'Ted' & wiki_data$LastName == 'Cruz'))

# set the reference scrores for AOC and Cruz and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

#Variation 7: Jerry Nadler-Michele Bachmann
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of Jerry Nadler and Michele Bachmann wiki content
dem_index <- which((wiki_data$FirstName == 'Jerry' & wiki_data$LastName == 'Nadler'))
rep_index <- which((wiki_data$FirstName == 'Michele' & wiki_data$LastName == 'Bachmann'))

# set the reference scrores for Nadler and Bachmann and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

#Variation 8: Jerry Nadler-Orrin Hatch
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of Jerry Nadler and Orrin Hatch wiki content
dem_index <- which((wiki_data$FirstName == 'Jerry' & wiki_data$LastName == 'Nadler'))
rep_index <- which((wiki_data$FirstName == 'Orrin' & wiki_data$LastName == 'Hatch'))

# set the reference scrores for Nadler and Hatch and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

#Variation 9: Jerry Nadler-Ted Cruz
```{r}
wiki_data <- as.data.frame(docvars(wiki_corpus))

# find the index of Jerry Nadler and Ted Cruz wiki content
dem_index <- which((wiki_data$FirstName == 'Jerry' & wiki_data$LastName == 'Nadler'))
rep_index <- which((wiki_data$FirstName == 'Ted' & wiki_data$LastName == 'Cruz'))

# set the reference scrores for Nadler and Cruz and leave the rest to NA 
refscores <- rep(NA, nrow(wiki_dfm))
refscores[rep_index] <- -1
refscores[dem_index] <- 1

#Fit a wordscores model using the reference scores
wordscores_model <- textmodel_wordscores(wiki_dfm, refscores, scale = "linear", smooth = 1)

# Extract the wordscores, rescale them and then save in wiki_data data-frame we created earlier
wordscores <- predict(wordscores_model, rescaling = "mv")
wordscores

```

#ggplots for the wordscore exercise
#Plot Sanders - Bachmann
```{r}
S_BCH<- read_excel("Wordscores.xlsx", sheet = "Sanders Bachmann", range = "A1:C12")
S_BCH$party <- as.factor(S_BCH$party)

ggplot(S_BCH, aes(x=score, y=Legislator, color = party)) +
  geom_point() + xlab('Wordscores') +  ylab('legislators') +
  scale_colour_manual(values = c("blue", "red"))
```

#ggplots for the wordscore exercise
#Plot Sanders - Hatch
```{r}
SH <- read_excel("Wordscores.xlsx", sheet = "SH", range = "A1:C31")
SH$party <- as.factor(SH$party)

ggplot(SH, aes(x=score, y=Legislator, color = party)) +
  geom_point() + xlab('Wordscores') +  ylab('legislators') +
  scale_colour_manual(values = c("blue", "red")) 
```

#ggplots for the wordscore exercise
#Plot AOC - Bachmann
```{r}
AOC_BCH <- read_excel("Wordscores.xlsx", sheet = "AOC BCH", range = "A1:C31")
AOC_BCH$party <- as.factor(AOC_BCH$party)

ggplot(AOC_BCH, aes(x=score, y=Legislator, color = party)) +
  geom_point() + xlab('Wordscores') +  ylab('legislators') +
  scale_colour_manual(values = c("blue", "red"))
```

